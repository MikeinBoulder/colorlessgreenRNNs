visited_classes=visited_classes
)
)
}
}
# Add the classes we found in this call and all recursive calls to the list of
# discovered classes.
full_classes_list <- c(full_classes_list, classes_list, sub_classes)
}
# Return the classes we found, removing any duplicates.
return(unique(full_classes_list))
}
# Main call to do clustering
classes <- get_classes(
values,
variability_scalar=VARIABILITY_SCALAR,
constrain_pcs=CONSTRAIN_INITIAL_PARTITION_PCS,
constrain_partition_number=CONSTRAIN_INITIAL_PARTITION_NUMBER
)
# Print the results to console
for (i in 1:length(classes)) {
print(classes[[i]])
}
if (SAVE_CLASSES) {
# Write the results to a file and print them to the console
file_prefix <- basename(DATA_ROOT)
filename <- paste(OUTPUT_DIRECTORY, file_prefix, FILE_SUFFIX, '.classes', sep="")
handle <-file(filename, 'w+')
# This line writes up all the segments into the first line of the file
writeLines(paste(as.character(sounds[1,]), collapse=' '), handle)
for (i in 1:length(classes)) {
writeLines(paste(classes[[i]], collapse=' '), handle)
}
close(handle)
}
foods = c('mac', 'paomien', 'burrito', 'chowmien')
sample(foods, 1)
foods = c('porkchop', 'paella', 'chowmien', 'linguini')
sample(foods, 1)
foods = c('porkchop', 'chowmien', 'linguini')
sample(foods, 1)
foods = c('dumplings', 'porkchop', 'lanzhou', 'fuzhou')
sample(foods, 1)
foods = c('linguini', 'cod', 'burrito')
sample(foods, 1)
library(Ckmeans.1d.dp)
# The path to and base filename of the .data, .sounds, and .contexts files to read.
DATA_ROOT <- "C:/Users/Sheng-Fu/Documents/GitHub/distributional_learning/vector_data/catalan-phoible"
# Whether or not we save the discovered classes to a file.
SAVE_CLASSES = TRUE
# Where to save the list of discovered classes
OUTPUT_DIRECTORY = "C:/Users/Sheng-Fu/Documents/GitHub/distributional_learning/output/"
# Optional suffix to add to the names of saved files
FILE_SUFFIX = ""
# A parameter that controls what amount of variance a principal component must account for to
# be used in clustering. The threshold is VARIABILITY_SCALAR * (average amount of variance).
VARIABILITY_SCALAR <- 0.5
# A parameter that controls what amount of variance a principal component must account for to
# be used in clustering. The threshold is VARIABILITY_SCALAR * (average amount of variance).
VARIABILITY_SCALAR <- 1
# A parameter that, if TRUE, sets restrictions on the initial partition of the data set: namely,
# any partition of the full set of sounds must be into two classes (e.g. consonants vs. vowels,
# voiced vs. voiceless, etc.)
CONSTRAIN_INITIAL_PARTITION_NUMBER <- TRUE
# A parameter that, if TRUE, restricts the initial partition of the data set: namely,
# only the first principal component is considered. Setting this to FALSE will result
# in the same classes being detected as when it is TRUE, but with additional partitions
# of the data set potentially discovered as well. Similar results can be gained by
# increasing VARIABILITY_SCALAR, but this will apply to all recursive calls to the clusterer
# rather than just the top level call.
CONSTRAIN_INITIAL_PARTITION_PCS <- TRUE
# Read files
values <- read.csv(paste(DATA_ROOT,".data",sep=""), sep=" ", header=FALSE)
values
sounds <- read.csv(paste(DATA_ROOT,".sounds",sep=""), sep=" ", header=FALSE,
stringsAsFactors=FALSE, colClasses = c("character"))
contexts <- read.csv(paste(DATA_ROOT,".contexts",sep=""), sep=" ", header=FALSE)
rownames(values) <- t(sounds)
colnames(values) <- t(contexts)
get_classes <- function(input_data,
constrain_pcs=FALSE,
constrain_partition_number=FALSE,
variability_scalar=1,
visited_classes=list()) {
# input_data: the matrix to factor and cluster
# constrain_pcs: if TRUE, this partition of the entire set of sounds will be
#                restricted to only the first PC. Otherwise all PCs up to the
#                variance threshold will be explored.
# constraint_partition_number: if TRUE, the partition of the entire set of
#                              sounds will be forced to be into two classes
#                              (e.g. consonants and vowels). If FALSE, any
#                              number of classes between 1 and 3 may be
#                              retrieved.
# variability scalar: Scales the amount of variance that a PC must capture to be
#                     used for clustering. For example, 1 indicates that a PC
#                     must capture at least the average amount of variance of
#                     all PCs to be used. 2 means it must capture twice the average
#                     amount of variance, etc.
# visited_classes: Keeps track of which classes we've already produced to avoid
#                  unnecessary duplication of effort. You shouldn't need to use this.
full_classes_list <- list()
local_sounds <- row.names(input_data)
# Do a PCA on the input data.
pca_data <- prcomp(input_data, center=TRUE)
if (constrain_pcs) {
highest_dim <- 1
}
else {
# If we're looking at all PCs, calculate which ones we will examine
# based on Kaiser's stopping criterion. Cluster into between 1-3 clusters.
mean_sdev <- mean(pca_data$sdev) * variability_scalar
highest_dim <- max(1, min(which(pca_data$sdev <  mean_sdev)) - 1)
}
if (constrain_partition_number) {
# Only cluster into a maximum of two classes.
cluster_range <- c(1,2)
}
else {
# Cluster into a maximum of three classes.
cluster_range <- c(1,3)
}
# Go through all the PCs we want to cluster over
for (i in 1:highest_dim) {
classes_list <- list()
sub_classes <- list()
col <- pca_data$x[,i]
# Do 1D k-means clustering on this PC
# Ckmeans.1d.dp issues a warning when the maximum number of clusters is smaller
# than the length of the list of points it's clustering. Suppress it for sanity.
result <- suppressWarnings(Ckmeans.1d.dp(col, cluster_range))
# Add each of the discovered clusters to the list of discovered clusters
num_clusters <- max(result$cluster)
for (j in 1:num_clusters) {
classes_list <- c(classes_list, list(local_sounds[result$cluster == j]))
}
# Go through all the discovered classes and see if we should cluster over them.
for (j in 1:length(classes_list)) {
class_vec <- classes_list[[j]]
# Check that the class contains more than two objects and is a subset of the input sounds
meaningful_partition <- length(class_vec) > 2 && length(class_vec) < length(local_sounds)
# Check that we haven't already clustered this subset. This isn't strictly necessary,
# but saves some cycles.
visited <- list(class_vec) %in% visited_classes
if (meaningful_partition && !visited) {
# If this class looks worth partitioning, flag it as seen and apply the clustering
# algorithm recursively.
visited_classes <- c(visited_classes, list(class_vec))
sub_classes <- c(
sub_classes,
get_classes(
input_data[class_vec,],
variability_scalar=variability_scalar,
visited_classes=visited_classes
)
)
}
}
# Add the classes we found in this call and all recursive calls to the list of
# discovered classes.
full_classes_list <- c(full_classes_list, classes_list, sub_classes)
}
# Return the classes we found, removing any duplicates.
return(unique(full_classes_list))
}
# Main call to do clustering
classes <- get_classes(
values,
variability_scalar=VARIABILITY_SCALAR,
constrain_pcs=CONSTRAIN_INITIAL_PARTITION_PCS,
constrain_partition_number=CONSTRAIN_INITIAL_PARTITION_NUMBER
)
# Print the results to console
for (i in 1:length(classes)) {
print(classes[[i]])
}
if (SAVE_CLASSES) {
# Write the results to a file and print them to the console
file_prefix <- basename(DATA_ROOT)
filename <- paste(OUTPUT_DIRECTORY, file_prefix, FILE_SUFFIX, '.classes', sep="")
handle <-file(filename, 'w+')
# This line writes up all the segments into the first line of the file
writeLines(paste(as.character(sounds[1,]), collapse=' '), handle)
for (i in 1:length(classes)) {
writeLines(paste(classes[[i]], collapse=' '), handle)
}
close(handle)
}
foods = c('dumplings', 'cod', 'linguini')
sample(foods, 1)
foods = c('nocchi', 'linguini', 'bento')
sample(foods, 1)
foods = c('nocchi', 'linguini', 'bento')
sample(foods, 1)
foods = c('nocchi', 'linguini', 'other')
sample(foods, 1)
foods = c('halal', 'fuzhou', 'porkchop', 'lanzhou')
sample(foods, 1)
foods = c('bento', 'linguini', 'dumplings', 'noodles')
sample(foods, 1)
foods = c('linguini', 'lanzhou', 'porkchop', 'bento')
sample(foods, 1)
foods = c('butterfly', 'linguini', 'curry', 'paella')
sample(foods, 1)
foods = c('curry', 'ravioli', 'lanchou', 'porkchop', 'fuzhou')
sample(foods, 1)
foods = c('curry', 'ravioli', 'italian dumpling')
sample(foods, 1)
foods = c('dumplings', 'fuzhou', 'califlower', 'ravioli', 'paomien')
sample(foods, 1)
foods = c('burrito', 'ravioli', 'paomien', 'butterfly', 'bento')
sample(foods, 1)
library(tidyverse)
results_ph = read_tsv("C:/Users/Sheng-Fu/Google Drive/Projects/Dissertation/results/PH_gl.tsv")
results_arti = read_tsv("C:/Users/Sheng-Fu/Google Drive/Projects/Dissertation/results/arti_gl.tsv")
results_ph$type = "Attested"
results_arti$type = "Random Artificial"
results = rbind(results_ph, results_arti)
results = results[results$id != 2155,]
results$prob_loc = results$loc/results$possible_pairs
results = results[is.na(results$prob_loc) == F,]
results_for_graph = group_by(results, spec, method, type) %>%
summarise(mean_glob = mean(glob), sd_glob = sd(loc)/sqrt(n()),
mean_loc = mean(loc), sd_loc = sd(loc)/sqrt(n()),
mean_prob_loc = mean(prob_loc), sd_prob_loc = sd(prob_loc)/sqrt(n()))
results_for_graph
results_for_graph$method = gsub("(py|eo|om)$", "\\1 (PHOIBLE)", results_for_graph$method)
p_glob = ggplot(results_for_graph, aes(x = spec, y = mean_glob, fill = method)) +
geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=mean_glob-sd_glob, ymax=mean_glob+sd_glob), width=.2,
position=position_dodge(.9)) +
ylab("glob") + facet_grid(~type) +
theme(text = element_text(size=15), axis.text.x = element_text(size = 20),
strip.text.x = element_text(size = 20)) +
scale_fill_grey()
#scale_fill_hue(h.start = 20)
p_glob
results_for_graph
p_ploc = ggplot(results_for_graph, aes(x = spec, y = mean_prob_loc, fill = method)) +
geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=mean_prob_loc-sd_prob_loc, ymax=mean_prob_loc+sd_prob_loc), width=.2,
position=position_dodge(.9)) +
ylab("loc") +facet_grid(~type) +
theme(text = element_text(size=15), axis.text.x = element_text(size = 20),
strip.text.x = element_text(size = 20))+
scale_fill_grey()
#scale_fill_hue(h.start = 20)
p_ploc
foods = c('bento', 'lanzhou', 'fuzhou', 'porkchop')
sample(foods, 1)
foods = c('squash', 'burrito', 'paella', 'lasnia')
sample(foods, 1)
library(tidyverse)
library(jsonlite)
setwd("C:/Users/Sheng-Fu/Desktop/outputs")
two_prefix_lstm <- stream_in(file("blimp-lstm_twoprefix_peephole.jsonl"))
one_prefix_lstm <- stream_in(file("blimp-lstm_oneprefix_peephole.jsonl"))
lm_lstm <- stream_in(file("blimp-lstm_simplelm_peephole.jsonl"))
lm_lstm$linguistics_term = gsub("s-selection", "argument_structure", lm_lstm$linguistics_term)
two_prefix_lstm$linguistics_term = gsub("s-selection", "argument_structure", two_prefix_lstm$linguistics_term)
one_prefix_lstm$linguistics_term = gsub("s-selection", "argument_structure", one_prefix_lstm$linguistics_term)
colnames(one_prefix_lstm)
one_prefix_lstm$one_prefix_prob1 = one_prefix_lstm$prefix_logits1 + one_prefix_lstm$crit_logits1
one_prefix_lstm$one_prefix_prob2 = one_prefix_lstm$prefix_logits2 + one_prefix_lstm$crit_logits2
one_prefix_lstm$pred = one_prefix_lstm$one_prefix_prob2 > one_prefix_lstm$one_prefix_prob1
lm_lstm$pred = lm_lstm$lm_prob2 > lm_lstm$lm_prob1
colnames(two_prefix_lstm)
two_prefix_lstm$two_prefix_prob1 = two_prefix_lstm$prefix_logits1 + two_prefix_lstm$crit_logits1
two_prefix_lstm$two_prefix_prob2 = two_prefix_lstm$prefix_logits2 + two_prefix_lstm$crit_logits2
two_prefix_lstm$pred = two_prefix_lstm$two_prefix_prob2 > two_prefix_lstm$two_prefix_prob1
one_prefix_results = select(one_prefix_lstm, UID, pairID, linguistics_term, pred)
two_prefix_results = select(two_prefix_lstm, UID, pairID, linguistics_term, pred)
lm_results = select(lm_lstm, UID, pairID, linguistics_term, pred)
one_prefix_results$type = "one-prefix"
lm_results$type = "sentence"
two_prefix_results$type = "two-prefix"
broad_results = rbind(lm_results, one_prefix_results, two_prefix_results)
broad_breakdown = group_by(broad_results, UID, linguistics_term, type) %>% summarise(m_pred = mean(pred))
broad_breakdown
write_tsv(broad_breakdown, "lstm_broad_breakdown.tsv")
cat_12 = group_by(broad_breakdown, linguistics_term, type) %>% filter(type == 'sentence') %>%  summarise(m_pred = mean(m_pred))
cat_12
write_tsv(cat_12, "lstm_cat_12_breakdown.tsv")
group_by(broad_breakdown, type) %>% filter(type == 'sentence') %>%  summarise(m_pred = mean(m_pred))
colnames(one_prefix_lstm)
one_prefix_lstm$lm_pred = one_prefix_lstm$lm_prob1 < one_prefix_lstm$lm_prob2
one_prefix_lstm$append_pred = one_prefix_lstm$append_logits1 < one_prefix_lstm$append_logits2
one_prefix_lstm$crit_pred = one_prefix_lstm$crit_logits1 < one_prefix_lstm$crit_logits2
one_prefix_lstm$append_ent_pred = one_prefix_lstm$appen_entropy1 < one_prefix_lstm$appen_entropy2
one_prefix_lstm_breakdown = group_by(one_prefix_lstm, linguistics_term, UID) %>%
summarise(m_lm_pred = mean(lm_pred),
m_append_pred = mean(append_pred),
m_crit_pred = mean(crit_pred),
m_append_ent_pred = mean(append_ent_pred),
m_pred = mean(pred))
one_prefix_lstm$lm_pred_cat = as.factor(one_prefix_lstm$lm_pred)
ggplot(one_prefix_lstm, aes(x = (crit_logits2 - crit_logits1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~linguistics_term)
one_prefix_lstm_binding = filter(one_prefix_lstm, linguistics_term == 'binding')
ggplot(one_prefix_lstm_binding, aes(x = (lm_prob2 - lm_prob1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point() + facet_wrap(~UID)
write_tsv(one_prefix_lstm_breakdown, "one_prefix_lstm_breakdown.tab")
two_prefix_lstm$lm_pred = two_prefix_lstm$lm_prob1 < two_prefix_lstm$lm_prob2
two_prefix_lstm$append_pred = two_prefix_lstm$append_logits1 < two_prefix_lstm$append_logits2
two_prefix_lstm$crit_pred = two_prefix_lstm$crit_logits1 < two_prefix_lstm$crit_logits2
two_prefix_lstm$append_ent_pred = two_prefix_lstm$appen_entropy1 < two_prefix_lstm$appen_entropy2
colnames(two_prefix_lstm)
two_prefix_lstm$lm_pred_cat = as.factor(two_prefix_lstm$lm_pred)
ggplot(two_prefix_lstm, aes(x = (two_prefix_prob2 - two_prefix_prob1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~linguistics_term)
ggplot(two_prefix_lstm, aes(x = (crit_logits2 - crit_logits1), y = (prefix_logits2 - prefix_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
unique(two_prefix_lstm$UID)
two_prefix_lstm_breakdown = group_by(two_prefix_lstm, UID, linguistics_term) %>%
summarise(m_lm_pred = mean(lm_pred),
m_append_pred = mean(append_pred),
m_crit_pred = mean(crit_pred),
m_append_ent_pred = mean(append_ent_pred),
m_pred = mean(pred))
two_prefix_lstm_breakdown
write_tsv(two_prefix_lstm_breakdown, "two_prefix_lstm_breakdown.tab")
###gpt2 results
one_prefix_gpt2 <- stream_in(file("blimp-gpt2_oneprefix.jsonl"))
two_prefix_gpt2 <- stream_in(file("blimp-gpt2_twoprefix.jsonl"))
lm_gpt2 <- stream_in(file("blimp-gpt2_simplelm.jsonl"))
colnames(one_prefix_gpt2)
one_prefix_gpt2$lm_pred = one_prefix_gpt2$lm_prob1 < one_prefix_gpt2$lm_prob2
colnames(lm_gpt2)
merge(lm_gpt2, one_prefix_gpt2, by = c("UID", "pairID"))
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2, two_prefix_gpt2, by = c("UID", "pairID"))
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2, by = c("UID", "pairID"))
merged_gpt2 = merge(merged_gpt2, two_prefix_gpt2, by = c("UID", "pairID"))
colnames(merged_gpt2)
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = c("UID", "pairID", "sent1_str", "sent2_str",
"appen_entropy1", "append_entropy2",
"crit_logits1", "crit_logits2",
"appen_logits1", "appen_logits2"))
colnames(lm_gpt2)
colnames(one_prefix_gpt2)
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = c("UID", "pairID", "sent1_str", "sent2_str"))
)
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = c("UID", "pairID", "sent1_str", "sent2_str"))
colnames(merged_gpt2)
colnames(two_prefix_gpt2)
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(one_prefix_gpt2)))
merged_gpt2 = merge(merged_gpt2, two_prefix_gpt2,
by = intersect(colnames(merged_gpt2), colnames(two_prefix_gpt2)))
colnames(merged_gpt2)
merged_gpt2$lm_pred = merged_gpt2$lm_prob1 < merged_gpt2$lm_prob2
merged_gpt2$append_pred = merged_gpt2$appen_logits1 < merged_gpt2$appen_logits2
merged_gpt2$crit_pred = merged_gpt2$crit_logits1 < merged_gpt2$crit_logits2
merged_gpt2$append_ent_pred = merged_gpt2$appen_entropy1 < merged_gpt2$appen_entropy2
colnames(merged_gpt2)
mean(merged_gpt2$lm_pred)
merged_gpt2$lm_pred
merged_gpt2$lm_prob1
merged_gpt2
merged_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(one_prefix_gpt2)))
merged_gpt2
View(merged_gpt2)
overall_gpt2 = merge(merged_gpt2, two_prefix_gpt2,
by = intersect(colnames(merged_gpt2), colnames(two_prefix_gpt2)))
overall_gpt2
merged_gpt2 = merge(lm_gpt2, two_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(two_prefix_gpt2)))
merged_gpt2_a = merge(lm_gpt2, one_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(one_prefix_gpt2)))
merged_gpt2_b = merge(lm_gpt2, two_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(two_prefix_gpt2)))
one_prefix_gpt2 = merge(lm_gpt2, one_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(one_prefix_gpt2)))
two_prefix_gpt2 = merge(lm_gpt2, two_prefix_gpt2,
by = intersect(colnames(lm_gpt2), colnames(two_prefix_gpt2)))
colnames(one_prefix_gpt2)
one_prefix_gpt2$lm_pred = one_prefix_gpt2$lm_prob1 < one_prefix_gpt2$lm_prob2
one_prefix_gpt2$append_pred = one_prefix_gpt2$appen_logits1 < one_prefix_gpt2$appen_logits2
one_prefix_gpt2$crit_pred = one_prefix_gpt2$crit_logits1 < one_prefix_gpt2$crit_logits2
one_prefix_gpt2$append_ent_pred = one_prefix_gpt2$appen_entropy1 < one_prefix_gpt2$appen_entropy2
ggplot(one_prefix_gpt2, aes(x = (crit_logits2 - crit_logits1), y = (prefix_logits2 - prefix_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
ggplot(one_prefix_gpt2, aes(x = (crit_logits2 - crit_logits1), y = (appen_logits2 - appen_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
one_prefix_gpt2$lm_pred_cat = as.factor(one_prefix_gpt2$lm_pred)
ggplot(one_prefix_gpt2, aes(x = (crit_logits2 - crit_logits1), y = (appen_logits2 - appen_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
one_prefix_gpt2$lm_pred = one_prefix_gpt2$lm_prob1 > one_prefix_gpt2$lm_prob2
one_prefix_gpt2$append_pred = one_prefix_gpt2$appen_logits1 > one_prefix_gpt2$appen_logits2
one_prefix_gpt2$crit_pred = one_prefix_gpt2$crit_logits1 > one_prefix_gpt2$crit_logits2
one_prefix_gpt2$append_ent_pred = one_prefix_gpt2$appen_entropy1 > one_prefix_gpt2$appen_entropy2
one_prefix_gpt2$lm_pred_cat = as.factor(one_prefix_gpt2$lm_pred)
ggplot(one_prefix_gpt2, aes(x = (crit_logits1 - crit_logits2), y = (appen_logits1 - appen_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
ggplot(one_prefix_lstm, aes(x = (crit_logits2 - crit_logits1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~linguistics_term)
ggplot(one_prefix_lstm, aes(x = (crit_logits2 - crit_logits1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
dim(one_prefix_lstm)
dim(one_prefix_gpt2)
ggplot(one_prefix_lstm, aes(x = (crit_logits2 - crit_logits1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Critical vs. Appendix logits")
ggplot(one_prefix_gpt2, aes(x = (crit_logits1 - crit_logits2), y = (appen_logits1 - appen_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("GPT2, Critical vs. Appendix logits")
colnames(two_prefix_gpt2)
two_prefix_gpt2$lm_pred = two_prefix_gpt2$lm_prob1 > two_prefix_gpt2$lm_prob2
two_prefix_gpt2$append_pred = two_prefix_gpt2$appen_logits1 > two_prefix_gpt2$appen_logits2
two_prefix_gpt2$crit_pred = two_prefix_gpt2$crit_logits1 > two_prefix_gpt2$crit_logits2
two_prefix_gpt2$append_ent_pred = two_prefix_gpt2$appen_entropy1 > two_prefix_gpt2$appen_entropy2
two_prefix_gpt2$pref_crit_logits1 = two_prefix_gpt2$crit_logits1 + two_prefix_gpt2$pref_logits1
two_prefix_gpt2$pref_crit_logits2 = two_prefix_gpt2$crit_logits2 + two_prefix_gpt2$pref_logits2
two_prefix_gpt2$lm_pred_cat = as.factor(two_prefix_gpt2$lm_pred)
ggplot(two_prefix_gpt2, aes(x = (pref_crit_logits1 - pref_crit_logits2), y = (appen_logits1 - appen_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("GPT2, Critical vs. Appendix logits")
ggplot(two_prefix_gpt2, aes(x = (pref_crit_logits1 - pref_crit_logits2), y = (appen_logits1 - appen_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("GPT2, Prefix + Critical vs. Appendix logits")
colnames(two_prefix_lstm)
two_prefix_lstm$pref_crit_logits1 = two_prefix_lstm$crit_logits1 + two_prefix_lstm$prefix_logits1
two_prefix_lstm$pref_crit_logits2 = two_prefix_lstm$crit_logits2 + two_prefix_lstm$prefix_logits2
ggplot(two_prefix_lstm, aes(x = (two_prefix_prob2 - two_prefix_prob1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~linguistics_term)
ggplot(two_prefix_lstm, aes(x = (two_prefix_prob2 - two_prefix_prob1), y = (append_logits2 - append_logits1), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (pref_logits1 - pref_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("GPT2, Prefix  vs. Critical logits")
ggplot(two_prefix_gpt2, aes(x = (crit_logits1 - crit_logits2), y = (pref_logits1 - pref_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("GPT2, Prefix  vs. Critical logits")
View(two_prefix_gpt2)
ggplot(two_prefix_lstm, aes(x = (two_prefix_prob1 - two_prefix_prob2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)  + ggtitle("LSTM, Prefix + Critical vs. Appendix logits")
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)  + ggtitle("LSTM, Prefix + Critical vs. Appendix logits")
setwd("~/GitHub/colorlessgreenRNNs/results/outputs")
library(tidyverse)
library(jsonlite)
two_prefix_lstm <- stream_in(file("blimp-lstm_twoprefix_peephole.jsonl"))
one_prefix_lstm <- stream_in(file("blimp-lstm_oneprefix_peephole.jsonl"))
lm_lstm <- stream_in(file("blimp-lstm_simplelm_peephole.jsonl"))
#load results in tab
#two_prefix_lstm = read_tsv('blimp-lstm_twoprefix_peephole.tab')
#lm_lstm = read_tsv('blimp-lstm_simplelm_peephole.tab')
#one_prefix_lstm = read_tsv('blimp-lstm_oneprefix_peephole.tab')
lm_lstm$linguistics_term = gsub("s-selection", "argument_structure", lm_lstm$linguistics_term)
two_prefix_lstm$linguistics_term = gsub("s-selection", "argument_structure", two_prefix_lstm$linguistics_term)
one_prefix_lstm$linguistics_term = gsub("s-selection", "argument_structure", one_prefix_lstm$linguistics_term)
colnames(one_prefix_lstm)
one_prefix_lstm$one_prefix_prob1 = one_prefix_lstm$prefix_logits1 + one_prefix_lstm$crit_logits1
one_prefix_lstm$one_prefix_prob2 = one_prefix_lstm$prefix_logits2 + one_prefix_lstm$crit_logits2
one_prefix_lstm$pred = one_prefix_lstm$one_prefix_prob1 > one_prefix_lstm$one_prefix_prob2
lm_lstm$pred = lm_lstm$lm_prob2 > lm_lstm$lm_prob1
colnames(two_prefix_lstm)
#View(two_prefix_lstm)
two_prefix_lstm$two_prefix_prob1 = two_prefix_lstm$prefix_logits1 + two_prefix_lstm$crit_logits1
two_prefix_lstm$two_prefix_prob2 = two_prefix_lstm$prefix_logits2 + two_prefix_lstm$crit_logits2
two_prefix_lstm$pred = two_prefix_lstm$two_prefix_prob1 > two_prefix_lstm$two_prefix_prob2
one_prefix_results = select(one_prefix_lstm, UID, pairID, linguistics_term, pred)
two_prefix_results = select(two_prefix_lstm, UID, pairID, linguistics_term, pred)
lm_results = select(lm_lstm, UID, pairID, linguistics_term, pred)
one_prefix_results$type = "one-prefix"
lm_results$type = "sentence"
two_prefix_results$type = "two-prefix"
broad_results = rbind(lm_results, one_prefix_results, two_prefix_results)
broad_breakdown = group_by(broad_results, UID, linguistics_term, type) %>% summarise(m_pred = mean(pred))
broad_breakdown
write_tsv(broad_breakdown, "lstm_broad_breakdown.tsv")
cat_12 = group_by(broad_breakdown, linguistics_term, type) %>% filter(type == 'sentence') %>%  summarise(m_pred = mean(m_pred))
cat_12
write_tsv(cat_12, "lstm_cat_12_breakdown.tsv")
group_by(broad_breakdown, type) %>% filter(type == 'sentence') %>%  summarise(m_pred = mean(m_pred))
colnames(one_prefix_lstm)
one_prefix_lstm$lm_pred = one_prefix_lstm$lm_prob1 > one_prefix_lstm$lm_prob2
one_prefix_lstm$append_pred = one_prefix_lstm$append_logits1 >one_prefix_lstm$append_logits2
one_prefix_lstm$crit_pred = one_prefix_lstm$crit_logits1 > one_prefix_lstm$crit_logits2
one_prefix_lstm$append_ent_pred = one_prefix_lstm$appen_entropy1 > one_prefix_lstm$appen_entropy2
one_prefix_lstm_breakdown = group_by(one_prefix_lstm, linguistics_term, UID) %>%
summarise(m_lm_pred = mean(lm_pred),
m_append_pred = mean(append_pred),
m_crit_pred = mean(crit_pred),
m_append_ent_pred = mean(append_ent_pred),
m_pred = mean(pred))
#View(one_prefix_lstm_breakdown)
one_prefix_lstm$lm_pred_cat = as.factor(one_prefix_lstm$lm_pred)
ggplot(one_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Critical vs. Appendix logits")
one_prefix_lstm_binding = filter(one_prefix_lstm, linguistics_term == 'binding')
ggplot(one_prefix_lstm_binding, aes(x = (lm_prob1 - lm_prob2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point() + facet_wrap(~UID)
write_tsv(one_prefix_lstm_breakdown, "one_prefix_lstm_breakdown.tab")
two_prefix_lstm$lm_pred = two_prefix_lstm$lm_prob1 > two_prefix_lstm$lm_prob2
two_prefix_lstm$append_pred = two_prefix_lstm$append_logits1 > two_prefix_lstm$append_logits2
two_prefix_lstm$crit_pred = two_prefix_lstm$crit_logits1 > two_prefix_lstm$crit_logits2
two_prefix_lstm$append_ent_pred = two_prefix_lstm$appen_entropy1 > two_prefix_lstm$appen_entropy2
colnames(two_prefix_lstm)
two_prefix_lstm$lm_pred_cat = as.factor(two_prefix_lstm$lm_pred)
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)  + ggtitle("LSTM, Prefix + Critical vs. Appendix logits")
ggplot(two_prefix_lstm, aes(x = (two_prefix_prob1 - two_prefix_prob2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID)  + ggtitle("LSTM, Prefix + Critical vs. Appendix logits")
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (prefix_logits1 - prefix_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Prefix  vs. Critical logits") +
geom_line(aes = aes(x = x, y = y), colour = "red")
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (prefix_logits1 - prefix_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Prefix  vs. Critical logits") +
geom_abline(intercept = 0, slope = 1)
ggplot(two_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (prefix_logits1 - prefix_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Prefix  vs. Critical logits") +
geom_abline(intercept = 0, slope = -1)
ggplot(one_prefix_lstm, aes(x = (crit_logits1 - crit_logits2), y = (append_logits1 - append_logits2), color = lm_pred_cat)) +
geom_point(alpha = 0.2) + facet_wrap(~UID) + ggtitle("LSTM, Critical vs. Appendix logits") +
geom_abline(intercept = 0, slope = -1)
